Hadoop的三大核心组件：Hdfs、Yarn、MapReduce

Hdfs小文件问题：数据大小远小于默认数据块大小的文件。（实时性要求越高，小文件就会也多，跟量有关系，量越大，小文件越少，小文件问题越小）
            
            影响：存储层面：因为元数据存储于内存当中，大量小文件占用大量内存。
                 计算层面：每个小文件都会起一个MapTask,1个MapTask默认内存1G,浪费资源

    解决方式：入库前：数据采集或标准入库之前，将小文件进行合并大文件再上传入库
            存储：Hadoop Archive归档->将多个小文件打包成一个har文件，减少对NN内存的使用
            计算方面：CombineTextInputFormat用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片
            其他：


新老用户的报错：java.lang.NoClassDefFoundError: Could not initialize class com.alibaba.fastjson.util.TypeUtils
            解决方案：<dependency>
                            <groupId>com.alibaba</groupId>
                            <artifactId>fastjson</artifactId>
                            <version>2.0.3</version>
                    </dependency>